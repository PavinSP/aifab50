# Natural Language Processing
Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) and computer science that focuses on the interaction between computers and human language. It involves the development of algorithms and models that enable machines to understand, interpret, and generate human language in a way that is meaningful and contextually appropriate.
## Text Preprocessing
Text preprocessing involves transforming text into a clean and consistent format that can then be fed into a model for further analysis and learning. Text preprocessing techniques may be general so that they are applicable to many types of applications, or they can be specialized for a specific task.

Text preprocessing is a set of techniques used to prepare text data for further processing. It is an important step in natural language processing (NLP) because it can help to improve the accuracy and efficiency of NLP models.

Text preprocessing is a crucial step in Natural Language Processing (NLP) that involves cleaning and transforming raw text data into a format suitable for further analysis and modeling. It helps to enhance the quality and efficiency of NLP tasks and improves the performance of machine learning models.

- Stop Words:
    - Stop words removal is the data pre-processing step in the natural language processing (NLP) pipeline in which we remove all the highly frequent words from the text as it doesn't add any valuable information to understand the text better resulting in the NLP model dealing with less number of features.
    - Stop words are common words that do not add much meaning to the text. They are often removed from text data during text preprocessing because they can make the text more difficult to process and analyze. Some examples of stop words include:

       - the
       - is
       - and
       - of
       - to
       - in
       - that
       - with
       - as
- Tokenization:
    - Tokenization is a fundamental text preprocessing step in Natural Language Processing (NLP). It involves breaking down a piece of text, such as a sentence or a paragraph, into smaller units called tokens. These tokens can be individual words or subwords, depending on the tokenization method used. Tokenization is a necessary step for various NLP tasks as most NLP algorithms and models operate on the token level rather than on the raw text.
    - Tokenization is a process in natural language processing (NLP) that breaks a text into smaller units called tokens. These tokens can be words, phrases, or even individual characters. Tokenization is an essential step in many NLP tasks, such as sentiment analysis, text classification, and machine translation.
    - Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. The first step of the NLP process is gathering the data (a sentence) and breaking it into understandable parts (words).
- Stemming:
    - Stemming is a text preprocessing technique used in Natural Language Processing (NLP) to reduce words to their base or root form, called the stem. The stem represents the core meaning of a word and is obtained by removing suffixes or prefixes from the word. The goal of stemming is to reduce inflected words to a common form, so variations of a word can be treated as the same word, simplifying text analysis and improving efficiency in NLP tasks.
    - For example, with stemming, words like "running," "runs," and "ran" are all reduced to their common stem "run."
    - Stemming is a process in natural language processing (NLP) that reduces inflected words to their word stem, base or root form. Stemming is often used as a preprocessing step before applying other natural language processing tasks, such as information retrieval and text categorization.
    - Stemming is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as "lemmas". Stemming is important in natural language understanding (NLU) and natural language processing (NLP).
- Lemmatization:
    - Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good.
    - Lemmatization is a process in natural language processing (NLP) that reduces inflected words to their base or dictionary form, called a lemma. Lemmatization is similar to stemming, but it takes into account the context of the word. This means that lemmatization can sometimes produce more accurate results than stemming.
    - Lemmatization is a text preprocessing technique in Natural Language Processing (NLP) that aims to reduce words to their base or root form, known as the lemma. Unlike stemming, which simply chops off suffixes or prefixes to obtain the root, lemmatization involves analyzing words based on their morphological features and returning the base or dictionary form of a word, ensuring it is a valid word in the language.
    - The lemma represents the canonical form of a word, and lemmatization takes into account the word's part of speech (POS) to ensure accurate word reduction. For example, the lemma of "running" is "run," and the lemma of "better" is "good."
    - Lemmatization is often preferred over stemming for tasks that require a more accurate and linguistically valid word transformation. It helps to ensure that words with different inflections or variations are reduced to their common base form. This can be especially important for applications like text analysis, information retrieval, and search engines.
- Bag of words model:
    - The Bag-of-Words (BoW) model is a common and simple technique used in Natural Language Processing (NLP) for text representation. It treats text documents as unordered collections of words and ignores the word order and grammar in the text. The basic idea behind the BoW model is to represent each document as a numerical vector by counting the occurrences of each word (or token) in the document. These vectors are then used as input for various machine learning algorithms for tasks like text classification and sentiment analysis.
    - A bag-of-words (BoW) model is a simplified representation of text that considers only the presence or absence of words in a document. This means that the order of the words in the document is not considered. BoW models are often used in natural language processing (NLP) tasks such as text classification and sentiment analysis.
- N-grams:
    - An n-gram is a contiguous sequence of n words in a text. For example, a 2-gram is a sequence of two words, such as "the cat". N-grams are often used in natural language processing (NLP) to represent the structure of text.
    - In Natural Language Processing (NLP), n-grams are contiguous sequences of n items (words, characters, or tokens) extracted from a piece of text. N-grams are used to capture the local and contextual information within the text. They are an important concept in various NLP tasks, such as language modeling, text generation, and information retrieval.
- Word vectorizer:
    - In Natural Language Processing (NLP), a word vectorizer is a technique that converts words or tokens into numerical vectors. Word vectorization is a fundamental step in representing textual data in a format that can be processed by machine learning algorithms. These numerical representations are essential for training machine learning models for various NLP tasks.
    - A word vectorizer is a tool in natural language processing (NLP) that transforms text into numerical vectors. This allows computers to process and understand the meaning of text in a more meaningful way. Word vectorizers are often used in tasks such as sentiment analysis, machine translation, and question answering.
    - Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics. The process of converting words into numbers are called Vectorization.
- TF-IDF
    - TF-IDF (Term Frequency-Inverse Document Frequency) is a popular numerical representation technique used in Natural Language Processing (NLP) to convert text data into numerical vectors.
    - TF-IDF is calculated as follows:
        - TF-IDF = (Term Frequency) * (Inverse Document Frequency)
            - Term Frequency: This is the number of times a word appears in a document.
            - Inverse Document Frequency: This is the number of documents in the corpus that contain the word.
- POS tagging:
    - Part-of-speech (POS) tagging is a process in natural language processing (NLP) where each word in a text is labeled with its corresponding part of speech. This can include nouns, verbs, adjectives, and other grammatical categories.
    - Part-of-speech (POS) tagging is a natural language processing (NLP) task that assigns a part of speech to each word in a sentence. The part of speech of a word indicates its grammatical function in the sentence. For example, the word "dog" can be a noun (the animal), a verb (to chase), or an adjective (dog-eared).
    - The basic POS tags include:

        - Noun (NN): A word that represents a person, place, thing, or idea.
        - Verb (VB): A word that expresses an action, event, or state.
        - Adjective (JJ): A word that describes or modifies a noun.
        - Adverb (RB): A word that describes or modifies a verb, adjective, or other adverb.
        - Pronoun (PRP): A word used in place of a noun to avoid repetition.
        - Preposition (IN): A word that shows the relationship between nouns or pronouns and other words in a sentence.
        - Conjunction (CC): A word used to connect words, phrases, or clauses.
        - Determiner (DT): A word that introduces a noun and determines its specificity (e.g., articles like "the," "a," "an").
        - Interjection (UH): A word used to express emotions or sudden exclamations.
- Named entity recognition:
    - Named Entity Recognition (NER) is a natural language processing (NLP) task that identifies and classifies named entities in text. Named entities are typically people, organizations, locations, dates, times, and quantities. For example, the sentence "Barack Obama was born in Honolulu, Hawaii, on August 4, 1961" contains the named entities "Barack Obama", "Honolulu", "Hawaii", "August", "4", and "1961".
    - Named Entity Recognition (NER) is a Natural Language Processing (NLP) task that involves identifying and classifying named entities (e.g., names of people, organizations, locations, dates, monetary values, etc.) within a piece of text. The goal of NER is to extract and categorize these entities to understand the specific entities mentioned in the text and their respective types.
    - For example, in the sentence "Apple Inc. was founded by Steve Jobs on April 1, 1976, in Cupertino, California," NER would identify and classify the following named entities:
        - "Apple Inc." as an organization
        - "Steve Jobs" as a person
        - "April 1, 1976" as a date
        - "Cupertino" and "California" as locations
- Word embeddings:
    - Word embeddings are a type of representation for words that captures the meaning of the words in a way that computers can understand. 
    - Word embeddings are a type of numerical representation used in Natural Language Processing (NLP) to map words or tokens from a vocabulary to dense vectors of real numbers. These vectors capture the semantic meaning and relationships between words in a way that can be efficiently used for various NLP tasks.
    - Word Embeddings in NLP is a technique where individual words are represented as real-valued vectors in a lower-dimensional space and captures inter-word semantics. Each word is represented by a real-valued vector with tens or hundreds of dimensions.