# CNN ARCHITECTURES:
- LeNet:
    - Lenet-5 is one of the earliest pre-trained models for recognizing the handwritten and machine-printed characters.
- AlexNet:
    - AlexNet is a pioneering convolutional neural network (CNN) used primarily for image recognition and classification tasks.
    - The backpropagation algorithm is used to train AlexNet.
- VGGNet:
    - VGG stands for Visual Geometry Group; it is a standard deep Convolutional Neural Network (CNN) architecture with multiple layers. The “deep” refers to the number of layers with VGG-16 or VGG-19 consisting of 16 and 19 convolutional layers.
- InceptionNet:
    - InceptionNet is a relatively deep architecture, with 23 layers. It consists of 9 inception modules, 3 fully connected layers, and a global average pooling layer. The inception modules are a key innovation of InceptionNet. They allow the network to learn multiple features from the input data at the same time.
- ResNet:
    - Residual Network (ResNet) is a deep learning model used for computer vision applications. It is a Convolutional Neural Network (CNN) architecture designed to support hundreds or thousands of convolutional layers.
- EfficientNet:
    - EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient.
    - EfficientNet has been shown to be very effective for a variety of tasks. It has achieved state-of-the-art results on the ImageNet dataset, and it has been used for tasks such as object detection, natural language processing, and medical image analysis.
# TRANSFER LEARNING
Transfer learning is a machine learning technique where a model trained on a large dataset is reused as the starting point for a model on a new dataset. This can be useful when there is not enough data to train a model from scratch, or when you want to improve the performance of a model by using the knowledge that was learned from the previous model.
# DATA AUGMENTATION
Data augmentation is a technique used to artificially increase the size of a dataset by creating new data points from existing data points. This can be useful when there is not enough data to train a machine learning model, or when you want to improve the performance of a model by making it more robust to noise and changes in the data.

Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model, by training models on several slightly-modified copies of existing data.